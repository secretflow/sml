在 MPC（多方安全计算）的定点数（Fixed-point）环境下，为了防止数值溢出（Overflow）而对标签 $y$ 进行缩放（Scaling），是一个非常实用且常见的工程技巧。

结论先行：  
从理论上讲，对 $y$ 进行线性缩放（$y' \= y / K$）不会破坏 GLM 的核心预测能力，也不会导致预测偏差（Bias），前提是你关注的是点估计（Point Estimation，即预测均值 $\\mu$）。  
但是，这样做会改变**离散参数（Dispersion Parameter, $\\phi$）的解释，并对正则化参数（$\\lambda$）和收敛精度**产生影响。

以下是从 GLM 统计推断原理出发的详细分析：

### ---

**1\. 原理分析：指数族分布的标度不变性**

GLM 的核心假设是 $y$ 服从指数族分布（EDF），其方差满足 $Var(y) \= \\phi V(\\mu)$。  
我们来看看当你令 $y' \= c \\cdot y$ （其中 $c \= 1/y\_{max}$）时，各分布的数学性质发生了什么变化。  
令 $\\mu' \= E\[y'\] \= c\\mu$。

#### **(1) Gamma 分布与 Tweedie 分布（标度不变性）**

这是最完美的场景。

* Gamma 分布：方差 $Var(y) \= \\phi \\mu^2$。

  $$Var(y') \= c^2 Var(y) \= c^2 \\phi \\mu^2 \= \\phi (c\\mu)^2 \= \\phi (\\mu')^2$$

  结论：$y'$ 依然完美服从 Gamma 分布形式，且离散参数 $\\phi$ 保持不变。这也是为什么 Gamma 回归常用于金额预测，因为它对单位（元、万元）不敏感。  
* Tweedie 分布 ($1\<p\<2$)：方差 $Var(y) \= \\phi \\mu^p$。

  $$Var(y') \= c^2 \\phi \\mu^p \= c^2 \\phi (\\frac{\\mu'}{c})^p \= (c^{2-p}\\phi) (\\mu')^p$$

  结论：$y'$ 依然服从 Tweedie 分布形式，只是离散参数从 $\\phi$ 变成了 $\\phi' \= c^{2-p}\\phi$。这对预测均值没有影响，因为 GLM 求解 $\\beta$ 时 $\\phi$ 通常作为常数消掉。

#### **(2) 正态分布（Gaussian）**

* 方差 $Var(y) \= \\sigma^2$。

  $$Var(y') \= c^2 \\sigma^2$$

  结论：依然是正态分布，仅仅是方差缩小了 $c^2$ 倍。完全没问题。

#### **(3) 泊松分布（Poisson）—— 需要特别注意**

* **理论冲突**：标准泊松分布定义在非负整数 $\\{0, 1, 2, ...\\}$ 上，且方差 $Var(y) \= \\mu$（即 $\\phi=1$）。  
* 缩放后果：如果 $y$ 是整数，$y' \= y/1000$ 就变成了小数。且：

  $$Var(y') \= c^2 \\mu \= c (c\\mu) \= c \\mu'$$

  这不再是标准的泊松分布（要求 $Var=\\mu'$），而是变成了 Quasi-Poisson（拟泊松） 分布，其离散参数 $\\phi \= c$。  
* **实际影响**：在 GLM 求解系数 $\\beta$ 的过程中（无论是 IRLS 还是 SGD），我们最大化的是拟似然（Quasi-likelihood）。**只要方差函数 $V(\\mu) \= \\mu$ 的结构保持不变，点估计 $\\hat{\\beta}$ 依然是无偏的**。  
  * *简单说*：即使 $y'$ 变成了 0.001, 0.002... 这种小数，你依然可以用 Poisson Loss 训练，预测出的均值再乘回去是完全准确的。

### ---

**2\. 对 Link Function 和系数 $\\beta$ 的影响**

我们需要知道缩放 $y$ 后，模型学到的系数 $\\beta'$ 发生了什么变化，以便确认是否能恢复。

假设原模型：$\\eta \= X\\beta, \\quad \\mu \= g^{-1}(\\eta)$。  
缩放后模型：$\\eta' \= X\\beta', \\quad \\mu' \= c\\mu$。

#### **情况 A：Log Link (常用，如 Tweedie/Gamma/Poisson)**

$$\\log(\\mu') \= \\log(c\\mu) \= \\log(c) \+ \\log(\\mu) \= \\log(c) \+ X\\beta$$

* 结论：缩放 $y$ 只改变截距项（Intercept）。

  $$\\beta'\_0 \= \\beta\_0 \+ \\log(c)$$  
  $$\\beta'\_j \= \\beta\_j \\quad (j \> 0)$$

  这非常棒，意味着特征的权重（斜率）在数值上不需要改变，这对定点数计算非常友好。

#### **情况 B：Identity Link (如 Linear Regression)**

$$\\mu' \= c\\mu \\implies X\\beta' \= c(X\\beta)$$

* 结论：所有系数整体缩小 $c$ 倍。

  $$\\beta' \= c \\beta$$

#### **情况 C：Logit Link (二分类)**

通常二分类标签是 0/1，不需要缩放。如果针对连续比例值（如 0\~1 之间的概率）做回归，逻辑同 Log Link。

### ---

**3\. 潜在风险与解决方案（MPC 特有）**

虽然理论上可行，但在 MPC 工程实现中必须注意以下三点：

#### **(1) 正则化系数 $\\lambda$ 的失配 (The Regularization Mismatch)**

这是最容易踩的坑。  
目标函数通常是：$\\mathcal{L}(\\beta) \+ \\lambda \\|\\beta\\|^2$。

* 当你把 $y$ 缩小 $c$ 倍（例如 $10^{-6}$），Likelihood 部分的梯度也会大幅缩小。  
* 但是，如果你不相应地缩小 $\\lambda$，正则化项的梯度就会相对变大，导致模型被**过度惩罚（Over-regularized）**，预测出的值会趋向于 0（Underfitting）。  
* **修正方案**：如果 $y$ 缩小了 $c$ 倍，建议根据具体的 Loss 推导，同步调整 $\\lambda$。  
  * 对于 Linear Regression (Identity Link): $\\lambda\_{new} \\approx c^2 \\lambda\_{old}$。  
  * 对于 Log Link: 情况复杂一些，因为 $\\beta$ 大小没变，但 Gradient 变小了。通常需要调小 $\\lambda$。

#### **(2) 定点数精度 (Precision Loss)**

* **下溢风险**：如果 $y$ 本身就很小，再除以 $y\_{max}$，可能导致 $y'$ 在定点数表示下变成 0 或精度极低。  
* **梯度消失**：如果 $y'$ 极小，反向传播的梯度也会极小，导致 SGD 更新不动。  
* **建议**：  
  * 不要无脑除以 $y\_{max}$。如果 $y\_{max}$ 很大但大部分数据很小（长尾分布），可以考虑除以 $p99$ 分位数，或者取对数 $\\log(y+1)$ 作为预处理（注意这就改变了分布假设，变成了 Log-Normal）。  
  * 在 MPC 中，确保缩放后的 $y'$ 依然落在定点数的“甜点区”（例如 $10^{-2} \\sim 10^2$ 之间），不要太小。

#### **(3) 恢复时的误差放大**

预测结束后，你需要执行 $\\hat{y} \= \\hat{y}' \\times y\_{max}$。

* 在 MPC 中，这一步乘法如果 $y\_{max}$ 很大，可能会再次导致溢出，或者将 $\\hat{y}'$ 中的量化噪声放大 $y\_{max}$ 倍。  
* **建议**：最后的恢复步骤建议在**明文**下进行（即 Reveal $\\hat{y}'$ 给接收方，由接收方在本地乘 $y\_{max}$），或者确保 SPU 的输出位宽足够大。

### ---

**4\. 总结与建议**

**你的方案是可行的，且符合统计学原理。**

**操作建议：**

1. **预处理**：执行 $y' \= y / \\text{scale}$。  
2. **分布设置**：  
   * **Gamma/Tweedie/Normal**：直接训练，无需修改分布逻辑。  
   * **Poisson**：可以训练，但在文档中注明这是 Quasi-Poisson 回归，允许非整数输入。  
3. **Link 选择**：  
   * 首选 **Log Link**。因为它具有“斜率不变性”，特征权重的数值范围不会因为 $y$ 的缩放而剧烈波动，非常适合定点数系统。  
4. **超参调整**：  
   * 提醒用户，如果开启了 Scaling，其传入的 l2\_norm ($\\lambda$) 可能需要相应调小，否则模型会欠拟合。或者你在算法内部根据 scale 自动缩放 $\\lambda$。  
5. **后处理**：  
   * 预测结果 $\\hat{y}'$ 最好 Reveal 给用户后，由用户在本地乘回 scale，既节省 MPC 算力，又避免最后一步溢出。