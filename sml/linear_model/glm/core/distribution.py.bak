# Copyright 2024 Ant Group Co., Ltd.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from abc import ABC, abstractmethod

import jax
import jax.numpy as jnp

from .link import IdentityLink, Link, LogitLink, LogLink


class Tweedie(Distribution):
    """
    Tweedie distribution.
    A compound Poisson-Gamma distribution for modeling non-negative data.

    The Tweedie distribution includes:
    - Normal (p=0)
    - Poisson (p=1)
    - Gamma (p=2)
    - InverseGaussian (p=3)

    For 1 < p < 2, it's a compound Poisson-Gamma distribution.

    Parameters
    ----------
    power : float
        The variance power parameter p.
        Must be: p <= 0, p = 1, p >= 2, or 1 < p < 2
    """

    def __init__(self, power: float = 1.5):
        self.power = power
        # Validate power parameter
        if not (power <= 0 or power == 1 or power >= 2 or (1 < power < 2)):
            raise ValueError(
                f"Tweedie power must be <= 0, = 1, >= 2, or between 1 and 2. Got {power}"
            )

    def unit_variance(self, mu: jax.Array) -> jax.Array:
        # V(mu) = mu^p
        mu = jnp.clip(mu, 1e-7, 1e14)
        return jnp.power(mu, self.power)

    def deviance(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # Deviance for Tweedie depends on power parameter
        mu = jnp.clip(mu, 1e-7, 1e14)
        y = jnp.maximum(y, 1e-10)  # Ensure y > 0
        eps = 1e-10

        p = self.power

        if abs(p - 0) < 1e-10:  # Normal
            dev = (y - mu) ** 2
        elif abs(p - 1) < 1e-10:  # Poisson
            dev = 2.0 * (y * jnp.log(y / mu + eps) - (y - mu))
        elif abs(p - 2) < 1e-10:  # Gamma
            dev = 2.0 * ((y - mu) / mu - jnp.log(y / mu + eps))
        elif abs(p - 3) < 1e-10:  # Inverse Gaussian
            dev = (y - mu) ** 2 / (mu ** 2 * y)
        elif 1 < p < 2:  # Compound Poisson-Gamma
            # Use approximation for compound distribution
            # This is the generalized deviance
            dev = 2.0 * (
                jnp.maximum(y, 0) ** (2 - p) / ((2 - p) * mu ** (1 - p))
                - y ** (2 - p) / ((1 - p) * mu ** (2 - p))
                + mu ** (2 - p) / ((2 - p) * (1 - p))
            )
        else:
            # General case using approximation
            dev = jnp.abs(y - mu) ** (2 - p) / (mu ** (1 - p))

        return jnp.sum(dev * weights) if weights is not None else jnp.sum(dev)

    def log_likelihood(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # Log-likelihood approximation for Tweedie
        mu = jnp.clip(mu, 1e-7, 1e14)
        y = jnp.maximum(y, 1e-10)

        p = self.power

        if abs(p - 0) < 1e-10:  # Normal
            ll = -0.5 * (y - mu) ** 2 - 0.5 * jnp.log(2 * jnp.pi)
        elif abs(p - 1) < 1e-10:  # Poisson
            ll = y * jnp.log(mu) - mu
        elif abs(p - 2) < 1e-10:  # Gamma
            ll = -y / mu - jnp.log(mu)
        else:
            # Approximation for other powers
            ll = -(y ** (2 - p)) / ((2 - p) * mu ** (1 - p))

        return jnp.sum(ll * weights) if weights is not None else jnp.sum(ll)

    def starting_mu(self, y: jax.Array) -> jax.Array:
        # R-style initialization
        # For Tweedie, we need to ensure positivity
        mu_start = (y + jnp.mean(y)) / 2.0
        if self.power > 1:  # Ensure positive mu for powers > 1
            mu_start = jnp.maximum(mu_start, 1e-7)
        return mu_start

    def get_canonical_link(self) -> Link:
        # Canonical link depends on power
        p = self.power
        if abs(p - 0) < 1e-10:
            return IdentityLink()
        elif abs(p - 1) < 1e-10:
            return LogLink()
        elif abs(p - 2) < 1e-10:
            # For Gamma, canonical link is inverse (1/mu)
            # But inverse can be numerically unstable
            # We'll implement it as ReciprocalLink
            return ReciprocalLink()
        elif abs(p - 3) < 1e-10:
            # For Inverse Gaussian, canonical link is 1/mu^2
            # We'll use PowerLink(power=-2)
            return PowerLink(power=-2.0)
        else:
            # For other powers, use LogLink as a stable default
            return LogLink()


class Distribution(ABC):
    """
    Abstract base class for GLM distributions.

    Provides methods to compute the unit variance function V(mu),
    deviance, log-likelihood, and starting values for the mean.
    """

    @abstractmethod
    def unit_variance(self, mu: jax.Array) -> jax.Array:
        """
        Compute the unit variance function V(mu).

        The variance of the distribution is given by: Var(Y) = phi * V(mu),
        where phi is the dispersion parameter (scale).

        Note on a(phi):
        In the exponential family form f(y; theta, phi) = exp((y*theta - b(theta))/a(phi) + c),
        typically a(phi) = phi / sample_weight.
        This class provides V(mu) = b''(theta).
        """
        pass

    @abstractmethod
    def deviance(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        """
        Compute the deviance of the distribution.

        Deviance is a measure of goodness of fit, defined as:
        D = 2 * (LogLikelihood(Saturated Model) - LogLikelihood(Proposed Model))

        Parameters
        ----------
        y : jax.Array
            The target values.
        mu : jax.Array
            The predicted mean values.
        weights : jax.Array, optional
            Sample weights. If None, defaults to 1.

        Returns
        -------
        deviance : jax.Array
            The computed total deviance (scalar).
        """
        pass

    @abstractmethod
    def log_likelihood(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        """
        Compute the log-likelihood of the distribution.

        Used for calculating information criteria like AIC and BIC.

        Parameters
        ----------
        y : jax.Array
            The target values.
        mu : jax.Array
            The predicted mean values.
        weights : jax.Array, optional
            Sample weights.

        Returns
        -------
        ll : jax.Array
            The computed total log-likelihood (scalar).
        """
        pass

    @abstractmethod
    def starting_mu(self, y: jax.Array) -> jax.Array:
        """
        Compute robust starting values for the mean mu.

        These are used to initialize the IRLS or Newton solver.

        Parameters
        ----------
        y : jax.Array
            The target values.

        Returns
        -------
        mu_start : jax.Array
            Starting values for mu.
        """
        pass

    @abstractmethod
    def get_canonical_link(self) -> Link:
        """
        Return the default canonical link function for this distribution.

        Returns
        -------
        link : Link
            The canonical link instance.
        """
        pass


class Normal(Distribution):
    """
    Normal (Gaussian) distribution.
    Canonical link: Identity.
    """

    def unit_variance(self, mu: jax.Array) -> jax.Array:
        # V(mu) = 1
        return jnp.ones_like(mu)

    def deviance(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # Deviance = sum(weights * (y - mu)^2)
        dev = (y - mu) ** 2
        return jnp.sum(dev * weights) if weights is not None else jnp.sum(dev)

    def log_likelihood(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # LogLikelihood ~ -0.5 * sum((y - mu)^2) (ignoring constants and dispersion for basic LL)
        # Note: For exact AIC/BIC comparison, constant terms like log(2*pi) are included.
        ll = -0.5 * (y - mu) ** 2 - 0.5 * jnp.log(2 * jnp.pi)
        return jnp.sum(ll * weights) if weights is not None else jnp.sum(ll)

    def starting_mu(self, y: jax.Array) -> jax.Array:
        # R-style initialization: (y + mean(y)) / 2
        return (y + jnp.mean(y)) / 2.0

    def get_canonical_link(self) -> Link:
        return IdentityLink()


class Bernoulli(Distribution):
    """
    Bernoulli distribution (for binary classification).
    Canonical link: Logit.
    """

    def unit_variance(self, mu: jax.Array) -> jax.Array:
        # V(mu) = mu * (1 - mu)
        return mu * (1.0 - mu)

    def deviance(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # Deviance formula for Bernoulli
        # D = 2 * sum(y * log(y/mu) + (1-y) * log((1-y)/(1-mu)))
        # Added small epsilon to denominators to prevent log(0) and division by zero.
        mu = jnp.clip(mu, 1e-7, 1 - 1e-7)
        eps = 1e-10
        # Use a numerically stable formulation:
        # When y=1, term is 2*log(1/mu). When y=0, term is 2*log(1/(1-mu)).
        # Here we follow the standard formulation with eps.
        dev = 2.0 * (
            y * jnp.log(y / mu + eps)
            + (1.0 - y) * jnp.log((1.0 - y) / (1.0 - mu) + eps)
        )
        return jnp.sum(dev * weights) if weights is not None else jnp.sum(dev)

    def log_likelihood(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # LL = y * log(mu) + (1-y) * log(1-mu)
        mu = jnp.clip(mu, 1e-7, 1 - 1e-7)
        ll = y * jnp.log(mu) + (1.0 - y) * jnp.log(1.0 - mu)
        return jnp.sum(ll * weights) if weights is not None else jnp.sum(ll)

    def starting_mu(self, y: jax.Array) -> jax.Array:
        # Start with (y + 0.5) / 2 to avoid boundary issues [0, 1]
        return (y + 0.5) / 2.0

    def get_canonical_link(self) -> Link:
        return LogitLink()


class Poisson(Distribution):
    """
    Poisson distribution (for count data).
    Canonical link: Log.
    """

    def unit_variance(self, mu: jax.Array) -> jax.Array:
        # V(mu) = mu
        return mu

    def deviance(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # Deviance formula for Poisson
        # D = 2 * sum(y * log(y/mu) - (y - mu))
        # Clip mu to ensure positiveness
        mu = jnp.clip(mu, 1e-7, 1e14)
        eps = 1e-10
        dev = 2.0 * (y * jnp.log(y / mu + eps) - (y - mu))
        return jnp.sum(dev * weights) if weights is not None else jnp.sum(dev)

    def log_likelihood(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # LL = y * log(mu) - mu - log(y!)
        # We omit log(y!) here as it is constant w.r.t parameters.
        mu = jnp.clip(mu, 1e-7, 1e14)
        ll = y * jnp.log(mu) - mu
        return jnp.sum(ll * weights) if weights is not None else jnp.sum(ll)

    def starting_mu(self, y: jax.Array) -> jax.Array:
        # R-style initialization: (y + mean(y)) / 2
        return (y + jnp.mean(y)) / 2.0

    def get_canonical_link(self) -> Link:
        return LogLink()


class Gamma(Distribution):
    """
    Gamma distribution (for continuous positive data).
    Canonical link: Inverse (often Log is used in practice).
    """

    def unit_variance(self, mu: jax.Array) -> jax.Array:
        # V(mu) = mu^2
        return mu**2

    def deviance(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # Deviance = 2 * sum( -log(y/mu) + (y - mu)/mu )
        #          = 2 * sum( (y - mu)/mu - log(y/mu) )
        mu = jnp.clip(mu, 1e-7, 1e14)
        eps = 1e-10
        dev = 2.0 * ((y - mu) / mu - jnp.log(y / mu + eps))
        return jnp.sum(dev * weights) if weights is not None else jnp.sum(dev)

    def log_likelihood(
        self, y: jax.Array, mu: jax.Array, weights: jax.Array | None = None
    ) -> jax.Array:
        # LL depends on dispersion phi (which we often don't know exactly during training).
        # Standard form: LL(mu; y, phi) = -1/phi * (y/mu + log(mu)) + ...
        # For simplified LL maximization/comparison, we can use the main term:
        # - (y/mu + log(mu))
        mu = jnp.clip(mu, 1e-7, 1e14)
        ll = -1.0 * (y / mu + jnp.log(mu))
        return jnp.sum(ll * weights) if weights is not None else jnp.sum(ll)

    def starting_mu(self, y: jax.Array) -> jax.Array:
        # R-style initialization: (y + mean(y)) / 2
        mu_start = (y + jnp.mean(y)) / 2.0
        # Gamma requires positive values
        return jnp.maximum(mu_start, 1e-7)

    def get_canonical_link(self) -> Link:
        # The canonical link is Inverse, but inverse is numerically unstable.
        # Log link is often preferred. However, adhering to "Canonical" definition:
        # We need InverseLink implemented first?
        # For now, let's use LogLink as a safe default if Inverse is not available,
        # or implement InverseLink. Assuming InverseLink will be added or use Log as pragmatic canonical.
        # R uses Inverse. Let's return LogLink for stability until InverseLink is robust.
        return LogLink()  # TODO: Switch to InverseLink when available and robust
