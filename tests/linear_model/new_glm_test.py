# Copyright 2024 Ant Group Co., Ltd.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import time

import jax
import jax.numpy as jnp
import numpy as np
import pytest
from sml.linear_model.glm.core.distribution import (
    Normal,
    Bernoulli,
    Poisson,
    Gamma,
    Tweedie,
    NegativeBinomial,
    InverseGaussian,
)
from sml.linear_model.glm.core.link import (
    IdentityLink,
    LogLink,
    LogitLink,
    ProbitLink,
    CLogLogLink,
    PowerLink,
    ReciprocalLink,
)
from sml.linear_model.glm.formula.dispatch import register_formula
from sml.linear_model.glm.formula.optimized import PoissonLogFormula
from sml.linear_model.glm.model import GLM


class TestJAXGLM:
    def setup_method(self):
        # Generate synthetic Poisson data
        key = jax.random.PRNGKey(42)
        self.n_samples = 200
        self.n_features = 5

        key, subkey = jax.random.split(key)
        self.X = jax.random.normal(subkey, (self.n_samples, self.n_features))

        # True coefficients
        self.true_coef = jnp.array([0.5, -0.3, 0.2, 0.1, -0.1])
        self.true_intercept = 1.0

        # Linear predictor
        eta = self.X @ self.true_coef + self.true_intercept

        # Mean mu = exp(eta)
        mu = jnp.exp(eta)

        # Generate y (Poisson distributed)
        key, subkey = jax.random.split(key)
        self.y = jax.random.poisson(subkey, mu).astype(jnp.float32)

    def test_poisson_generic_jit_naive_inv(self):
        """Test Poisson GLM with Generic Formula & Naive Inv IRLS."""

        print("\n--- Testing IRLS (Naive Inv) ---")
        start_time = time.time()

        @jax.jit
        def train_and_predict(X, y):
            # IRLS solver uses naive inversion now
            model = GLM(dist=Poisson(), solver="irls", fit_intercept=True, tol=1e-6)
            model.fit(X, y)
            return model.coef_, model.intercept_

        coef, intercept = train_and_predict(self.X, self.y)
        end_time = time.time()

        print(f"Time: {end_time - start_time:.4f}s")
        print("Coef:", coef)
        print("Intercept:", intercept)

        # Check correctness
        np.testing.assert_allclose(coef, self.true_coef, rtol=0.2, atol=0.2)
        np.testing.assert_allclose(intercept, self.true_intercept, rtol=0.2, atol=0.2)

    def test_poisson_sgd_jit_with_decay(self):
        """Test Poisson GLM with SGD Solver and LR Decay."""

        print("\n--- Testing SGD Solver with Decay ---")

        # Need careful tuning for SGD on Poisson
        lr = 1e-2
        epochs = 200
        batch_size = 32

        start_time = time.time()

        @jax.jit
        def train_and_predict(X, y):
            model = GLM(
                dist=Poisson(),
                solver="sgd",
                learning_rate=lr,
                decay_rate=0.9,  # Decay LR every 500 steps
                decay_steps=500,
                max_iter=epochs,
                batch_size=batch_size,
                fit_intercept=True,
                tol=1e-6,
            )
            model.fit(X, y)
            return model.coef_, model.intercept_

        coef, intercept = train_and_predict(self.X, self.y)
        end_time = time.time()

        print(f"Time: {end_time - start_time:.4f}s")
        print("Coef:", coef)
        print("Intercept:", intercept)

        # SGD might be less precise than IRLS with limited epochs
        np.testing.assert_allclose(coef, self.true_coef, rtol=0.3, atol=0.3)
        np.testing.assert_allclose(intercept, self.true_intercept, rtol=0.3, atol=0.3)

    def test_offset_handling(self):
        """Test that offset is correctly handled in fit and predict."""
        print("\n--- Testing Offset Handling ---")

        # Create a large offset that would change predictions significantly
        offset = jnp.ones_like(self.y) * 2.0

        # Fit model with offset
        # Effectively, we are fitting y ~ Poisson(exp(X@beta + intercept + offset))
        # The true intercept should now be (true_intercept - 2.0) approx if offset was capturing part of it?
        # No, if we pass offset, the model learns the residual.

        model = GLM(dist=Poisson(), solver="irls")
        model.fit(self.X, self.y, offset=offset)

        # Since true data was generated with intercept=1.0 and offset=0
        # Now we force offset=2.0. The model should compensate by learning intercept approx -1.0
        # exp(1.0) = exp(-1.0 + 2.0)

        print("Intercept with offset=2.0:", model.intercept_)
        np.testing.assert_allclose(
            model.intercept_, self.true_intercept - 2.0, atol=0.2
        )

        # Test predict with offset
        mu_pred = model.predict(self.X, offset=offset)
        dev = model.evaluate(self.X, self.y, metrics=["deviance"], offset=offset)[
            "deviance"
        ]
        print("Deviance with offset:", dev)

        # It should be close to the original model's deviance
        assert dev < 300  # Heuristic check

    def test_metrics(self):
        """Test extended metrics evaluation."""
        model = GLM(dist=Poisson(), solver="irls")
        model.fit(self.X, self.y)

        metrics = model.evaluate(self.X, self.y, metrics=["deviance", "aic", "rmse"])
        print("\nMetrics:", metrics)

        assert "deviance" in metrics
        assert "aic" in metrics
        assert "rmse" in metrics
        assert metrics["rmse"] > 0

    def test_sgd_batching(self):
        """Verify SGD runs with different batch sizes."""
        for bs in [10, 50, 200]:  # 200 is full batch
            print(f"\nTesting SGD batch_size={bs}")
            model = GLM(dist=Poisson(), solver="sgd", batch_size=bs, max_iter=10)
            model.fit(self.X, self.y)
            assert model.coef_ is not None

    def test_y_scaling(self):
        """Test manual y scaling for numerical stability."""
        print("\n--- Testing Y Scaling ---")

        # Calculate scale
        y_max = float(jnp.max(self.y))
        y_scale = y_max / 2.0
        if y_scale < 1.0:
            y_scale = 1.0

        print(f"Using y_scale: {y_scale}")

        # Fit with scale
        model = GLM(dist=Poisson(), solver="irls", fit_intercept=True)
        model.fit(self.X, self.y, scale=y_scale)

        # The intercept should change because y is scaled.
        # For Log link: log(y/scale) = log(y) - log(scale)
        # So intercept should be reduced by log(scale)
        expected_intercept_shift = jnp.log(y_scale)
        print(
            f"Learned Intercept: {model.intercept_}, Expected Shift: -{expected_intercept_shift}"
        )

        # Compare with unscaled model
        model_unscaled = GLM(dist=Poisson(), solver="irls")
        model_unscaled.fit(self.X, self.y)

        print(f"Unscaled Intercept: {model_unscaled.intercept_}")

        # intercept_scaled ~ intercept_unscaled - log(scale)
        np.testing.assert_allclose(
            model.intercept_,
            model_unscaled.intercept_ - expected_intercept_shift,
            atol=0.2,
        )

        # Coefficients should be identical (scale only affects intercept in Log link)
        np.testing.assert_allclose(model.coef_, model_unscaled.coef_, atol=1e-4)

        # Prediction with scale should match original y
        mu_pred = model.predict(self.X, scale=y_scale)
        mu_unscaled = model_unscaled.predict(self.X)

        np.testing.assert_allclose(mu_pred, mu_unscaled, rtol=1e-4)
        print("Scaling test passed: predictions match unscaled model.")

    def test_tweedie_distribution(self):
        """Test Tweedie distribution with different power parameters."""
        print("\n--- Testing Tweedie Distribution ---")

        # Test Tweedie with power=1.5 (compound Poisson-Gamma)
        model = GLM(dist=Tweedie(power=1.5), solver="irls", fit_intercept=True)
        model.fit(self.X, self.y)
        print(f"Tweedie(1.5) - Coef: {model.coef_}, Intercept: {model.intercept_}")

        # Test Tweedie with power=0 (should behave like Normal)
        model_normal = GLM(dist=Tweedie(power=0.0), solver="irls", fit_intercept=True)
        model_normal.fit(self.X, self.y)
        print(
            f"Tweedie(0.0) - Coef: {model_normal.coef_}, Intercept: {model_normal.intercept_}"
        )

        # Test deviance computation
        mu = model.predict(self.X)
        dev = model.dist.deviance(self.y, mu)
        print(f"Tweedie deviance: {dev}")
        assert dev >= 0

    def test_negative_binomial_distribution(self):
        """Test Negative Binomial distribution with overdispersion."""
        print("\n--- Testing Negative Binomial Distribution ---")

        # Test with alpha=1.0
        model = GLM(dist=NegativeBinomial(alpha=1.0), solver="irls", fit_intercept=True)
        model.fit(self.X, self.y)
        print(
            f"NegativeBinomial(alpha=1.0) - Coef: {model.coef_}, Intercept: {model.intercept_}"
        )

        # Test with different alpha values
        for alpha in [0.5, 1.0, 2.0]:
            dist = NegativeBinomial(alpha=alpha)
            mu_test = jnp.array([1.0, 2.0, 3.0])
            var = dist.unit_variance(mu_test)
            print(f"NB alpha={alpha}: mu={mu_test}, var={var}")
            # Variance should be mu + alpha * mu^2
            expected_var = mu_test + alpha * mu_test**2
            np.testing.assert_allclose(var, expected_var, rtol=1e-5)

    def test_inverse_gaussian_distribution(self):
        """Test Inverse Gaussian distribution."""
        print("\n--- Testing Inverse Gaussian Distribution ---")

        model = GLM(dist=InverseGaussian(), solver="irls", fit_intercept=True)
        # Generate positive data suitable for Inverse Gaussian
        y_pos = jnp.maximum(self.y, 1.0)  # Ensure strictly positive
        model.fit(self.X, y_pos)
        print(f"InverseGaussian - Coef: {model.coef_}, Intercept: {model.intercept_}")

        # Test unit variance: V(mu) = mu^3
        dist = InverseGaussian()
        mu_test = jnp.array([0.5, 1.0, 2.0])
        var = dist.unit_variance(mu_test)
        expected_var = mu_test**3
        np.testing.assert_allclose(var, expected_var, rtol=1e-5)
        print(f"InverseGaussian unit variance test passed")

    def test_various_links(self):
        """Test different link functions."""
        print("\n--- Testing Various Link Functions ---")

        # Generate binary data for link testing
        key = jax.random.PRNGKey(123)
        X_binary = jax.random.normal(key, (100, 3))
        true_coef = jnp.array([1.0, -0.5, 0.5])
        true_intercept = 0.0
        eta = X_binary @ true_coef + true_intercept

        # Test different links with Bernoulli distribution
        links_to_test = [
            ("Logit", LogitLink()),
            ("Probit", ProbitLink()),
            ("CLogLog", CLogLogLink()),
        ]

        for link_name, link in links_to_test:
            model = GLM(dist=Bernoulli(), link=link, solver="irls", fit_intercept=True)
            # Generate y based on link
            mu = link.inverse(eta)
            key, subkey = jax.random.split(key)
            y_binary = jax.random.bernoulli(subkey, mu).astype(jnp.float32)

            model.fit(X_binary, y_binary)
            print(f"{link_name} Link - Coef: {model.coef_}")

            # Test link derivatives
            mu_test = jnp.array([0.2, 0.5, 0.8])
            link_val = link.link(mu_test)
            link_der = link.link_deriv(mu_test)
            inv_der = link.inverse_deriv(link_val)

            # For CLogLog, the derivative relationship is more complex
            # We'll just check that derivatives are non-zero where expected
            if link_name == "CLogLog":
                # CLogLog derivatives can be negative, that's expected
                assert not jnp.all(link_der == 0)
                assert not jnp.all(inv_der == 0)
            else:
                np.testing.assert_allclose(inv_der, 1.0 / link_der, rtol=1e-5)
            print(f"{link_name} derivative consistency test passed")

    def test_power_and_reciprocal_links(self):
        """Test PowerLink and ReciprocalLink."""
        print("\n--- Testing Power and Reciprocal Links ---")

        # Test PowerLink with different powers
        powers_to_test = [0.5, 1.0, 2.0, -1.0, -2.0]

        for power in powers_to_test:
            if abs(power) < 1e-10:
                continue  # Skip power=0 as it's not allowed
            link = PowerLink(power=power)

            mu_test = jnp.array([1.0, 2.0, 3.0])
            eta = link.link(mu_test)
            mu_recovered = link.inverse(eta)

            np.testing.assert_allclose(mu_recovered, mu_test, rtol=1e-5)
            print(f"PowerLink(power={power}) consistency test passed")

        # Test ReciprocalLink (should be same as PowerLink(power=-1))
        reciprocal = ReciprocalLink()
        power_neg_one = PowerLink(power=-1.0)

        mu_test = jnp.array([1.0, 2.0, 3.0])
        eta_recip = reciprocal.link(mu_test)
        eta_power = power_neg_one.link(mu_test)

        np.testing.assert_allclose(eta_recip, eta_power, rtol=1e-10)
        print("ReciprocalLink and PowerLink(power=-1) equivalence test passed")

    def test_link_distribution_combinations(self):
        """Test various distribution-link combinations."""
        print("\n--- Testing Distribution-Link Combinations ---")

        combinations = [
            (Normal(), IdentityLink()),
            (Normal(), LogLink()),
            (Poisson(), LogLink()),
            (Poisson(), PowerLink(power=0.5)),
            (Gamma(), LogLink()),
            (Gamma(), ReciprocalLink()),
            (Tweedie(power=1.5), LogLink()),
            (NegativeBinomial(alpha=0.5), LogLink()),
            (InverseGaussian(), PowerLink(power=-2.0)),
        ]

        for dist, link in combinations:
            model = GLM(
                dist=dist, link=link, solver="irls", fit_intercept=True, max_iter=10
            )
            try:
                model.fit(self.X, self.y)
                mu_pred = model.predict(self.X)
                print(f"{dist.__class__.__name__}+{link.__class__.__name__}: Success")
                assert mu_pred is not None
            except Exception as e:
                print(
                    f"{dist.__class__.__name__}+{link.__class__.__name__}: Error - {e}"
                )
                # Some combinations might be numerically unstable, that's expected

    def test_normal_distribution(self):
        """Test Normal (Gaussian) distribution with Identity link."""
        print("\n--- Testing Normal Distribution ---")

        # Generate Gaussian data
        key = jax.random.PRNGKey(42)
        n_samples, n_features = 100, 3
        X_normal = jax.random.normal(key, (n_samples, n_features))
        true_coef = jnp.array([1.0, -0.5, 0.3])
        true_intercept = 2.0
        key, subkey = jax.random.split(key)
        y_normal = (
            X_normal @ true_coef
            + true_intercept
            + 0.1 * jax.random.normal(subkey, (n_samples,))
        )

        # Test with IRLS
        model = GLM(dist=Normal(), solver="irls", fit_intercept=True, max_iter=20)
        model.fit(X_normal, y_normal)
        print(f"Normal + Identity - Coef: {model.coef_}, Intercept: {model.intercept_}")

        np.testing.assert_allclose(model.coef_, true_coef, rtol=0.1, atol=0.1)
        np.testing.assert_allclose(model.intercept_, true_intercept, rtol=0.1, atol=0.1)

        # Test with SGD
        model_sgd = GLM(
            dist=Normal(),
            solver="sgd",
            fit_intercept=True,
            max_iter=100,
            learning_rate=0.01,
        )
        model_sgd.fit(X_normal, y_normal)
        print(f"Normal + Identity (SGD) - Coef: {model_sgd.coef_}")

    def test_l2_regularization(self):
        """Test L2 regularization."""
        print("\n--- Testing L2 Regularization ---")

        # Without regularization
        model_no_reg = GLM(dist=Poisson(), solver="irls", l2=0.0)
        model_no_reg.fit(self.X, self.y)

        # With regularization
        model_with_reg = GLM(dist=Poisson(), solver="irls", l2=1.0)
        model_with_reg.fit(self.X, self.y)

        print(f"Coef without L2: {model_no_reg.coef_}")
        print(f"Coef with L2=1.0: {model_with_reg.coef_}")

        # Coefficients should be smaller with regularization (shrinkage effect)
        # Check that norm of coefficients is smaller
        norm_no_reg = jnp.linalg.norm(model_no_reg.coef_)
        norm_with_reg = jnp.linalg.norm(model_with_reg.coef_)
        print(f"Norm without L2: {norm_no_reg}, Norm with L2: {norm_with_reg}")
        assert (
            norm_with_reg < norm_no_reg
        ), "L2 regularization should shrink coefficients"

    def test_sample_weights(self):
        """Test sample weights."""
        print("\n--- Testing Sample Weights ---")

        # Create weights (emphasize first half of samples)
        weights = jnp.concatenate(
            [
                jnp.ones(self.n_samples // 2) * 2.0,
                jnp.ones(self.n_samples - self.n_samples // 2) * 0.5,
            ]
        )

        model_no_weights = GLM(dist=Poisson(), solver="irls")
        model_no_weights.fit(self.X, self.y)

        model_with_weights = GLM(dist=Poisson(), solver="irls")
        model_with_weights.fit(self.X, self.y, sample_weight=weights)

        print(f"Coef without weights: {model_no_weights.coef_}")
        print(f"Coef with weights: {model_with_weights.coef_}")

        # Coefficients should differ when weights are applied
        diff = jnp.linalg.norm(model_no_weights.coef_ - model_with_weights.coef_)
        print(f"Coefficient difference: {diff}")
        # Just check both models run successfully
        assert model_with_weights.coef_ is not None

    def test_bic_metric(self):
        """Test BIC metric evaluation."""
        print("\n--- Testing BIC Metric ---")

        model = GLM(dist=Poisson(), solver="irls")
        model.fit(self.X, self.y)

        metrics = model.evaluate(self.X, self.y, metrics=["deviance", "aic", "bic"])
        print(f"Metrics: {metrics}")

        assert "bic" in metrics
        # BIC should be larger than AIC for reasonable sample sizes
        # BIC = -2*LL + k*log(n), AIC = -2*LL + 2*k
        # For n > e^2 â‰ˆ 7.4, BIC > AIC
        assert metrics["bic"] > metrics["aic"], "BIC should be > AIC for n > 7"

    def test_formula_dispatcher(self):
        """Test custom formula registration and dispatch."""
        print("\n--- Testing Formula Dispatcher ---")
        from sml.linear_model.glm.formula.dispatch import dispatcher, register_formula
        from sml.linear_model.glm.formula.optimized import (
            PoissonLogFormula,
            NormalIdentityFormula,
            BernoulliLogitFormula,
            GammaLogFormula,
        )
        from sml.linear_model.glm.formula.generic import GenericFormula

        # Save original registry state
        original_registry = dispatcher._registry.copy()

        try:
            # Register custom formula instance
            register_formula(Poisson, LogLink, PoissonLogFormula())

            # Verify it's resolved correctly
            resolved = dispatcher.resolve(Poisson(), LogLink())
            assert isinstance(resolved, PoissonLogFormula)
            print("Formula dispatcher correctly resolved PoissonLogFormula")

            # Test that model uses the optimized formula
            model = GLM(dist=Poisson(), solver="irls")
            model.fit(self.X, self.y)
            assert model.coef_ is not None

            # Test that built-in optimized formulas are registered
            resolved_normal = dispatcher.resolve(Normal(), IdentityLink())
            assert isinstance(resolved_normal, NormalIdentityFormula)
            print("NormalIdentityFormula registered correctly")

            resolved_bernoulli = dispatcher.resolve(Bernoulli(), LogitLink())
            assert isinstance(resolved_bernoulli, BernoulliLogitFormula)
            print("BernoulliLogitFormula registered correctly")

            resolved_gamma_log = dispatcher.resolve(Gamma(), LogLink())
            assert isinstance(resolved_gamma_log, GammaLogFormula)
            print("GammaLogFormula registered correctly")

            # Test fallback to GenericFormula for unregistered pairs
            # Use a combination that doesn't have optimized formula
            resolved_generic = dispatcher.resolve(Normal(), LogLink())
            assert isinstance(resolved_generic, GenericFormula)
            print("Fallback to GenericFormula works correctly")
        finally:
            # Restore original registry to avoid affecting other tests
            dispatcher._registry = original_registry

    def test_convergence_history(self):
        """Test that history_ contains convergence information."""
        print("\n--- Testing Convergence History ---")

        model = GLM(dist=Poisson(), solver="irls", max_iter=50, tol=1e-6)
        model.fit(self.X, self.y)

        assert model.history_ is not None
        assert "n_iter" in model.history_
        assert "converged" in model.history_
        assert "final_deviance" in model.history_

        print(f"History: {model.history_}")

        # With enough iterations and reasonable tol, should converge
        # Note: converged is a JAX array, need to convert
        print(f"Converged: {model.history_['converged']}")
        print(f"Number of iterations: {model.history_['n_iter']}")

    def test_dispersion_estimation(self):
        """Test dispersion parameter estimation."""
        print("\n--- Testing Dispersion Estimation ---")

        # Poisson should have dispersion close to 1
        model_poisson = GLM(dist=Poisson(), solver="irls")
        model_poisson.fit(self.X, self.y)
        print(f"Poisson dispersion: {model_poisson.dispersion_}")

        # Generate overdispersed count data and test with NegativeBinomial
        key = jax.random.PRNGKey(123)
        X_nb = jax.random.normal(key, (100, 3))
        true_coef = jnp.array([0.3, -0.2, 0.1])
        eta = X_nb @ true_coef + 1.0
        mu = jnp.exp(eta)

        # Simulate overdispersed counts (add extra variance)
        key, subkey = jax.random.split(key)
        y_nb = jax.random.poisson(subkey, mu * 2).astype(jnp.float32)  # Overdispersed

        model_nb = GLM(dist=NegativeBinomial(alpha=0.5), solver="irls")
        model_nb.fit(X_nb, y_nb)
        print(f"NegativeBinomial dispersion: {model_nb.dispersion_}")

    def test_gamma_with_log_link(self):
        """Test Gamma distribution with Log link (business common)."""
        print("\n--- Testing Gamma + Log Link ---")

        # Generate Gamma-like positive data
        key = jax.random.PRNGKey(42)
        n_samples, n_features = 100, 3
        X_gamma = jax.random.normal(key, (n_samples, n_features))
        true_coef = jnp.array([0.5, -0.3, 0.2])
        true_intercept = 1.0
        eta = X_gamma @ true_coef + true_intercept
        mu = jnp.exp(eta)

        # Generate positive y values (approximate Gamma)
        key, subkey = jax.random.split(key)
        y_gamma = mu + 0.3 * mu * jax.random.normal(subkey, (n_samples,))
        y_gamma = jnp.abs(y_gamma) + 0.1  # Ensure positive

        # Test Gamma + Log (design doc recommends Log for business use)
        model = GLM(
            dist=Gamma(), link=LogLink(), solver="irls", fit_intercept=True, max_iter=20
        )
        model.fit(X_gamma, y_gamma)
        print(f"Gamma + Log - Coef: {model.coef_}, Intercept: {model.intercept_}")

        # Coefficients should be close to true values
        np.testing.assert_allclose(model.coef_, true_coef, rtol=0.3, atol=0.3)

    def test_optimized_formulas(self):
        """Test all optimized formula implementations."""
        print("\n--- Testing Optimized Formulas ---")
        from sml.linear_model.glm.formula.optimized import (
            NormalIdentityFormula,
            BernoulliLogitFormula,
            PoissonLogFormula,
            GammaReciprocalFormula,
            GammaLogFormula,
            TweedieLogFormula,
        )
        from sml.linear_model.glm.core.family import Family

        key = jax.random.PRNGKey(42)

        # Test 1: NormalIdentityFormula
        print("Testing NormalIdentityFormula...")
        X_normal = jax.random.normal(key, (50, 3))
        y_normal = X_normal @ jnp.array([1.0, -0.5, 0.3]) + 0.1 * jax.random.normal(
            key, (50,)
        )
        family_normal = Family(Normal(), IdentityLink())
        formula_normal = NormalIdentityFormula()
        beta_test = jnp.zeros(3)
        w, z_resid, mu, eta, dev, extras = formula_normal.compute_components(
            X_normal, y_normal, beta_test, None, family_normal
        )
        assert w.shape == (50,)
        assert jnp.all(w == 1.0)  # Normal+Identity has constant weights
        print("NormalIdentityFormula passed")

        # Test 2: BernoulliLogitFormula
        print("Testing BernoulliLogitFormula...")
        key, subkey = jax.random.split(key)
        y_binary = jax.random.bernoulli(subkey, 0.5, (50,)).astype(jnp.float32)
        family_bernoulli = Family(Bernoulli(), LogitLink())
        formula_logit = BernoulliLogitFormula()
        w, z_resid, mu, eta, dev, extras = formula_logit.compute_components(
            X_normal, y_binary, beta_test, None, family_bernoulli
        )
        assert w.shape == (50,)
        # W = mu * (1 - mu), for beta=0, mu=0.5, so W ~ 0.25
        np.testing.assert_allclose(w, 0.25 * jnp.ones(50), rtol=0.01)
        print("BernoulliLogitFormula passed")

        # Test 3: PoissonLogFormula
        print("Testing PoissonLogFormula...")
        family_poisson = Family(Poisson(), LogLink())
        formula_poisson = PoissonLogFormula()
        w, z_resid, mu, eta, dev, extras = formula_poisson.compute_components(
            X_normal, self.y[:50], beta_test, None, family_poisson
        )
        # For beta=0, mu=1, W=mu=1
        np.testing.assert_allclose(w, jnp.ones(50), rtol=0.01)
        print("PoissonLogFormula passed")

        # Test 4: GammaLogFormula
        print("Testing GammaLogFormula...")
        y_gamma = jnp.abs(y_normal) + 0.1
        family_gamma_log = Family(Gamma(), LogLink())
        formula_gamma_log = GammaLogFormula()
        w, z_resid, mu, eta, dev, extras = formula_gamma_log.compute_components(
            X_normal, y_gamma, beta_test, None, family_gamma_log
        )
        # Gamma + Log: W = 1 (constant)
        assert jnp.all(w == 1.0)
        print("GammaLogFormula passed")

        # Test 5: GammaReciprocalFormula
        print("Testing GammaReciprocalFormula...")
        beta_gamma = jnp.array([0.5, 0.3, 0.2])  # Non-zero to avoid singularity
        family_gamma_recip = Family(Gamma(), ReciprocalLink())
        formula_gamma_recip = GammaReciprocalFormula()
        w, z_resid, mu, eta, dev, extras = formula_gamma_recip.compute_components(
            X_normal, y_gamma, beta_gamma, None, family_gamma_recip
        )
        assert w.shape == (50,)
        print("GammaReciprocalFormula passed")

        # Test 6: TweedieLogFormula
        print("Testing TweedieLogFormula...")
        family_tweedie = Family(Tweedie(power=1.5), LogLink())
        formula_tweedie = TweedieLogFormula(power=1.5)
        w, z_resid, mu, eta, dev, extras = formula_tweedie.compute_components(
            X_normal, self.y[:50], beta_test, None, family_tweedie
        )
        # For beta=0, mu=1, W = mu^(2-p) = 1^0.5 = 1
        np.testing.assert_allclose(w, jnp.ones(50), rtol=0.01)
        print("TweedieLogFormula passed")

        print("All optimized formulas passed!")

    def test_r_style_initialization(self):
        """Test that R-style initialization is used in solvers."""
        print("\n--- Testing R-style Initialization ---")

        # For Poisson GLM, starting mu should be (y + mean(y)) / 2
        # This affects the initial beta via least squares fit

        # Create a model and check that it converges quickly
        # (R-style init should provide better starting point)
        model = GLM(dist=Poisson(), solver="irls", max_iter=10, tol=1e-6)
        model.fit(self.X, self.y)

        # With good initialization, should achieve reasonable fit in few iterations
        assert model.history_["n_iter"] <= 10
        print(f"IRLS converged in {model.history_['n_iter']} iterations")

        # Coefficients should be close to true values
        np.testing.assert_allclose(model.coef_, self.true_coef, rtol=0.3, atol=0.3)
        print("R-style initialization test passed")

    def test_logistic_regression_optimized(self):
        """Test logistic regression with optimized formula."""
        print("\n--- Testing Logistic Regression (Optimized) ---")

        # Generate binary classification data
        key = jax.random.PRNGKey(42)
        n_samples, n_features = 200, 4
        X_logit = jax.random.normal(key, (n_samples, n_features))
        true_coef = jnp.array([1.0, -0.5, 0.5, -0.3])
        true_intercept = 0.5
        eta = X_logit @ true_coef + true_intercept
        prob = jax.nn.sigmoid(eta)

        key, subkey = jax.random.split(key)
        y_binary = jax.random.bernoulli(subkey, prob).astype(jnp.float32)

        # Fit logistic regression
        model = GLM(
            dist=Bernoulli(), link=LogitLink(), solver="irls", fit_intercept=True
        )
        model.fit(X_logit, y_binary)
        print(f"Logistic - Coef: {model.coef_}, Intercept: {model.intercept_}")

        # Coefficients should be reasonably close to true values
        np.testing.assert_allclose(model.coef_, true_coef, rtol=0.5, atol=0.5)
        np.testing.assert_allclose(model.intercept_, true_intercept, rtol=0.5, atol=0.5)

        # Test predictions
        mu_pred = model.predict(X_logit)
        # Predictions should be probabilities between 0 and 1
        assert jnp.all(mu_pred >= 0) and jnp.all(mu_pred <= 1)
        print("Logistic regression (optimized) test passed")

    def test_tweedie_optimized_formula(self):
        """Test Tweedie distribution with optimized formula."""
        print("\n--- Testing Tweedie Optimized Formula ---")
        from sml.linear_model.glm.formula.optimized import TweedieLogFormula
        from sml.linear_model.glm.formula.dispatch import dispatcher, register_formula

        # Save original registry
        original_registry = dispatcher._registry.copy()

        try:
            # Register TweedieLogFormula with power=1.5
            power = 1.5
            register_formula(Tweedie, LogLink, TweedieLogFormula(power=power))

            # Fit Tweedie model
            model = GLM(dist=Tweedie(power=power), solver="irls", fit_intercept=True)
            model.fit(self.X, self.y)
            print(f"Tweedie(p={power}) - Coef: {model.coef_}")

            # Verify it uses the optimized formula
            from sml.linear_model.glm.formula.optimized import TweedieLogFormula

            resolved = dispatcher.resolve(Tweedie(power=power), LogLink())
            assert isinstance(resolved, TweedieLogFormula)
            print("Tweedie optimized formula test passed")
        finally:
            dispatcher._registry = original_registry

    def test_gamma_reciprocal_link(self):
        """Test Gamma distribution with canonical (Reciprocal) link."""
        print("\n--- Testing Gamma + Reciprocal Link (Canonical) ---")

        # Generate positive data
        key = jax.random.PRNGKey(42)
        n_samples, n_features = 100, 3
        X_gamma = jax.random.normal(key, (n_samples, n_features)) * 0.3

        # For Reciprocal link: eta = 1/mu, so mu = 1/eta
        # Keep eta positive by having positive linear predictor
        true_coef = jnp.array([0.1, 0.05, -0.05])
        true_intercept = 2.0  # Keep eta > 0 to ensure mu > 0
        eta = X_gamma @ true_coef + true_intercept
        mu_true = 1.0 / eta  # Reciprocal link inverse

        # Generate Gamma-like data
        key, subkey = jax.random.split(key)
        y_gamma = mu_true + 0.1 * mu_true * jax.random.normal(subkey, (n_samples,))
        y_gamma = jnp.abs(y_gamma) + 0.01  # Ensure positive

        # Fit model with Reciprocal link
        model = GLM(
            dist=Gamma(),
            link=ReciprocalLink(),
            solver="irls",
            fit_intercept=True,
            max_iter=50,
        )
        model.fit(X_gamma, y_gamma)
        print(
            f"Gamma + Reciprocal - Coef: {model.coef_}, Intercept: {model.intercept_}"
        )

        # Check that model fits (may not be exact due to Reciprocal link challenges)
        assert model.coef_ is not None
        print("Gamma + Reciprocal link test passed")


if __name__ == "__main__":
    t = TestJAXGLM()
    t.setup_method()
    t.test_poisson_generic_jit_naive_inv()
    t.test_poisson_sgd_jit_with_decay()
    t.test_offset_handling()
    t.test_metrics()
    t.test_sgd_batching()
    t.test_y_scaling()

    # Test new distributions and links
    t.test_tweedie_distribution()
    t.test_negative_binomial_distribution()
    t.test_inverse_gaussian_distribution()
    t.test_various_links()
    t.test_power_and_reciprocal_links()
    t.test_link_distribution_combinations()

    # New tests
    t.test_normal_distribution()
    t.test_l2_regularization()
    t.test_sample_weights()
    t.test_bic_metric()
    t.test_formula_dispatcher()
    t.test_convergence_history()
    t.test_dispersion_estimation()
    t.test_gamma_with_log_link()

    # New optimized formula tests
    t.test_optimized_formulas()
    t.test_r_style_initialization()
    t.test_logistic_regression_optimized()
    t.test_tweedie_optimized_formula()
    t.test_gamma_reciprocal_link()
